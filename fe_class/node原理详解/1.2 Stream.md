# Stream

流

shell 通过管道连接各个部分，输入和输出的规范是文本流.

Nodejs, 内置的Steam 模块实现了类似的功能，各个部分之间通过pipe().

```js
const Stream = require('steam');

const Readable = Stream.Readable;
const Writable = Stream.Writable;
const Duplex = Stream.Duplex;
const Transfrom = Stream.Transform;

const fs = require('fs');
fs.createReadStream('xxx.js').pipe(process.stdout);//首先创建可读流通过这种方式进行流式处理

```

## Readable
创建可读流
```js
// 流式的消耗迭代器

const Readable = require('stream').Readable;

class ToReadable extends Readable {
    constructor(iterator) {
        super();
        this.iterator = iterator;
    }

    _read() {
        const res = this.iterator.next();
        if (res.done) {
            // 数据源已经消耗完了，通过Push null 通知流。
            return this.push(null);
        }

        setTimeout(() => {
            this.push(res.value + '\n')
        }, 0);
    }
}

const iterator = function (limit) {
    return {
        next: function () {
            if (limit--) {
                return {
                    done: false,
                    value: limit + Math.random()
                };
            }
            return {
                done: true
            };
        }
    }
}(10000);

const readable = new ToReadable(iterator);

readable.on('data', data => process.stdout.write(data));

readable.on('end', () => process.stdout.write('DONE'));
```

有10000个随机数输出

创建可读流的时候，需要jichengReadable，并且实现_read方法

* _read是生产数据的逻辑
* 在_read方法中，通过调用push(data)将数据放入可读流中供下游消耗.
* 当全部数据生产完成后，push(null);
* 当可读流结束之后，不能再调用push(data);

可以通过监听data事件消耗可读流

* 在首次监听data事件后，readable会不断的调用_read方法产生数据
* 当所有的数据生产完毕，会触发end

## Writeable

```js
const Writable = require('stream').Writable;

const writable = Writable();


writable._write = function (data, enc, next) {
    // 这里用toString() 的原因是什么？
    process.stdout.write(data.toString().toUpperCase());
    process.nextTick(next)
    // next() 同步
}

writable.on('finish', () => process.stdout.write('DONE'));

writable.write('a' + '\n');
writable.write('b' + '\n');
writable.write('c' + '\n');

writable.end();
```

* 上游通过调用write方法写入数据到可写流中.
* 在_write中，当数据成功写入后，需要调用next告诉流开始处理下一个数据.
* 上游必须调用end方法来结束可写流
* end方法调用账号，会触发finish事件

### Tips
为什么一定要toString处理
因为可读流可写流它们的数据都是默认的buffer,所以我们需要用toString来转换成我们能够处理的字符串
为什么要调用next()// 或者说为什么要用process.nextTick(next)来调用
## Duplex

创建可读可写流

```js
const Duplex = require('stream').Duplex;

const duplex = Duplex();

duplex._read = function() {
    this._readNum = this._readNum || 0;
    if (this._readNum > 3) {
        this.push(null);
    } else {
        this.push(`${this._readNum++}`)
    }
}

duplex._write = function(buf, enc, next) {
    process.stdout.write(`_write ${buf.toString()}\n`);
    //stdout 对象是一个可读写的stream
    next();
}

duplex.on('data', data => console.log(`ondata = ${data.toString()}`));

duplex.write('a');
duplex.write('b');
duplex.write('c');
duplex.write('d');
duplex.write('e');

duplex.end();
```
## Transform

可读留中的数据 0, 1
可写流中的数据 a, b, c

在Transform中可写端写入的数据，经过自动变换后可以自动添加到可读端》

Transform extends Duplex, 实现了_read , _write , _transform 

## 数据类型

data.toString()

shell 管道连接上下游，标准的文本流

可读流：push(data) data: Sring | Buffer, 消耗data的时候，data事件输出的数据都是BUffer

可写流：write(data),data: String | Buffer, _write(data) Buffer

Node Stream 中的数据流都是Buffer类型