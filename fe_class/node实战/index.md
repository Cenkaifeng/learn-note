课前问题

Node.js 如何处理高并发？
通过cluster 开启多线程集群


# 使用Node.js实现一个带CLI的爬虫应用

命令行支持输入关键词，数量来下载百度搜索结果的图片

命令行支持输入关键词，数量 来下载百度搜索结果的视屏

## 什么是爬虫

自动浏览器，万维网的网络机器人

* 网路引擎使用爬虫更新自己的网站内容，以及对其他网站的缩影
* 个人使用爬虫去获取网站的内容

## 爬虫可以肆无忌惮的爬取所有的网站的内容吗？

不是的。

爬虫访问网站会消耗对方网站的流量、贷款、服务器资源。

robots.txt

1. 放在哪里？

存在王赞的根目录，是一个ascii编码的文件

2. robots.txt的作用是什么？

指定当前网站哪些内容能爬，哪些不能怕？

3. 允许所有爬虫？

User-agent: *
Disallow:

4. 允许指定的爬虫

User-agent: baidu_spider
Allow： 

5. 拦截所有爬虫

User-agent: *
Disallow: /


## 如何开始一个爬虫应用？

1. 确定要爬取的网站/页面

2. 分析网站的数据以及DOM结构

3. 确定技术选型

    * 模拟浏览器请求
        * request 已经不再维护这个库了
        * superagent 
    * 解析DOM
        * cheerio 类似jquery的api
        * jsdom 解析文本

模拟用户行为

puppeteer // 模拟用户行为发送无头请求。
常用截图、巡查（定期跑线上页面截图预期）
